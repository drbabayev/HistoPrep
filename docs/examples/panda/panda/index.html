<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><link rel="canonical" href="https://jopo666.github.io/histoprep/examples/panda/panda/" />
      <link rel="shortcut icon" href="../../../img/favicon.ico" />
    <title>Use case: PANDA dataset - HistoPrep</title>
    <link rel="stylesheet" href="../../../css/theme.css" />
    <link rel="stylesheet" href="../../../css/theme_extra.css" />
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.5.0/styles/github.min.css" />
        <link href="//use.fontawesome.com/releases/v5.8.1/css/all.css" rel="stylesheet" />
        <link href="//use.fontawesome.com/releases/v5.8.1/css/v4-shims.css" rel="stylesheet" />
        <link href="../../../css/mkapi-common.css" rel="stylesheet" />
        <link href="../../../css/mkapi-readthedocs.css" rel="stylesheet" />
    
      <script>
        // Current page data
        var mkdocs_page_name = "Use case: PANDA dataset";
        var mkdocs_page_input_path = "examples/panda/panda.md";
        var mkdocs_page_url = "/histoprep/examples/panda/panda/";
      </script>
    
    <script src="../../../js/jquery-3.6.0.min.js" defer></script>
    <!--[if lt IE 9]>
      <script src="../../../js/html5shiv.min.js"></script>
    <![endif]-->
      <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.5.0/highlight.min.js"></script>
      <script>hljs.initHighlightingOnLoad();</script> 
</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side stickynav">
    <div class="wy-side-scroll">
      <div class="wy-side-nav-search">
          <a href="../../.." class="icon icon-home"> HistoPrep
        </a><div role="search">
  <form id ="rtd-search-form" class="wy-form" action="../../../search.html" method="get">
      <input type="text" name="q" placeholder="Search docs" title="Type search term here" />
  </form>
</div>
      </div>

      <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../../..">Home</a>
                </li>
              </ul>
              <p class="caption"><span class="caption-text">About</span></p>
              <ul>
                  <li class="toctree-l1"><a class="reference internal" href="../../../about/install/">Install</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../../about/license/">License</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../../about/release_notes/">Release notes</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../../about/citation/">Citation</a>
                  </li>
              </ul>
              <p class="caption"><span class="caption-text">Examples</span></p>
              <ul class="current">
                  <li class="toctree-l1"><a class="reference internal" href="../../read_slides/read_slides/">Reading slides</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../cut_tiles/cut_tiles/">Saving tiles</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../dearray/dearray/">Dearraying TMA slides</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../preprocess/preprocess/">Preprocessing saved tile images</a>
                  </li>
                  <li class="toctree-l1 current"><a class="reference internal current" href="./">Use case: PANDA dataset</a>
    <ul class="current">
    </ul>
                  </li>
              </ul>
              <p class="caption"><span class="caption-text">API documentation</span></p>
              <ul>
                  <li class="toctree-l1"><a class="reference internal" href="../../../api/">histoprep</a>
                  </li>
              </ul>
      </div>
    </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">
      <nav class="wy-nav-top" role="navigation" aria-label="Mobile navigation menu">
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../..">HistoPrep</a>
        
      </nav>
      <div class="wy-nav-content">
        <div class="rst-content"><div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href="../../.." class="icon icon-home" alt="Docs"></a> &raquo;</li>
          <li>Examples &raquo;</li><li>Use case: PANDA dataset</li>
    <li class="wy-breadcrumbs-aside">
    </li>
  </ul>
  <hr/>
</div>

          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
            <div class="section" itemprop="articleBody">
              
                <h1 id="prostate-cancer-grade-assessment-panda">Prostate cANcer graDe Assessment (PANDA)</h1>
<p>In this use case, we're going to preprocess the PANDA dataset! This is a huge freely available dataset containing prostate cancer biopsies from two different medical centres. You can read more about the dataset in the <a href="https://www.nature.com/articles/s41591-021-01620-2">publication</a> or the <a href="https://www.kaggle.com/c/prostate-cancer-grade-assessment">kaggle competition</a>.</p>
<p>Easiest way to download the dataset is through the Kaggle API. Install kaggle API with</p>
<pre><code class="language-bash"># Install/upgrade the Kaggle API.
pip install kaggle --upgrade
# Then download the dataset.
mkdir -p ./PANDA/raw
kaggle competitions download -c prostate-cancer-grade-assessment -p ./PANDA/raw
</code></pre>
<p>Finally when the dataset has downloaded (and been unarchived), the directory structure is like this.</p>
<pre><code>./PANDA/
└──raw/
    ├── sample_submission.csv
    ├── test.csv
    ├── train.csv
    ├── train_images
    │   ├── 0005f7aaab2800f6170c399693a96917.tiff
    │   ├── 000920ad0b612851f8e01bcc880d9b3d.tiff 
    │   ├── 0018ae58b01bdadc8e347995b69f99aa.tiff
    │   ├── 001c62abd11fa4b57bf7a6c603a11bb9.tiff
    │   ├── 001d865e65ef5d2579c190a0e0350d8f.tiff
    │   └── ... [omitted 10 611 additional entries]
    └── train_label_masks
        ├── 0005f7aaab2800f6170c399693a96917_mask.tiff
        ├── 000920ad0b612851f8e01bcc880d9b3d_mask.tiff
        ├── 0018ae58b01bdadc8e347995b69f99aa_mask.tiff
        ├── 001c62abd11fa4b57bf7a6c603a11bb9_mask.tiff
        ├── 001d865e65ef5d2579c190a0e0350d8f_mask.tiff
        └── ... [omitted 10511 entries]
</code></pre>
<h1 id="cut-slide-images-into-small-tiles">Cut slide images into small tiles</h1>
<p>First thing to do is to cut the large slide images into smaller images, which can actually be fed to your amazing neural network! Luckily, this is easy with <code>SlideReader</code>. </p>
<pre><code class="language-python"># Read the slide.
reader = SlideReader(path=&quot;./PANDA/raw/train_images/00a7fb880dc12c5de82df39b30533da9.tiff&quot;)
# Exctract tile coordinates.
coordinates = reader.get_tile_coordinates(
    width=512,
    overlap=0.2,
    max_background=0.6,
)
# Then let's display a thumbnail image...
display(reader.thumbnail)
# ... the tissue mask...
display(reader.tissue_mask)
# .. and then an annotated thumbnail.
display(reader.annotated_thumbnail_tiles)
</code></pre>
<p><img alt="png" src="../output_2_0.png" /></p>
<p><img alt="png" src="../output_2_1.png" /></p>
<p><img alt="png" src="../output_2_2.png" /></p>
<p>From the above images we can see that the slide is read correctly, tissue mask seems correct, and we managed to cover the whole tissue section in tiles, without including tiles with too much background! Pretty good for a few lines of code.</p>
<p>Now can loop over the images in <code>train_images</code> folder and cut each image with the <code>SlideReader</code> function.</p>
<pre><code class="language-python"># NOTE: This cell is just an example and not actually excecuted.

import os
import pandas
from histoprep.helpers import progress_bar

combined_metadata = []
for f in progress_bar(os.scandir(&quot;./PANDA/raw/train_images/&quot;), desc=&quot;PANDA&quot;):
    # Read slide.
    reader = SlideReader(f.path)
    # Cut tiles.
    slide_metadata = reader.save_tiles(
        output_dir=&quot;PANDA/tiles&quot;,
        coordinates=reader.get_tile_coordinates(width=384, overlap=0.2, max_background=0.6),
    )
    combined_metadata.append(metadata)
# Combine metadata.
combined_metadata = pandas.stack(combined_metadata)
</code></pre>
<p>The above cell is a perfectly valid example, but it would be easier to cut all tiles with the <code>HistoPrep</code> excecutable installed alongside the <code>histoprep</code> python module.</p>
<pre><code class="language-python">!HistoPrep ./PANDA/raw/train_images ./PANDA/tiles 384 --overlap 0.2 --max_background 0.6 --ext tiff
</code></pre>
<p>Now that we have cut all slide images into tiles, we should remove any tiles which come from unwanted regions...</p>
<h1 id="finding-outliers">Finding outliers</h1>
<p>Slide images often contain areas, which we are not interested in. These areas might contain fingerprints, bubbles, ink, pen markings or some other shit. Detecting and identifying tiles from these areas is a crucial task, as neural networks might easily overfit these outliers and then give wrong predictions.</p>
<p>Let's start by combining the metadata from all of the processed slides.</p>
<pre><code class="language-python">import pandas
from histoprep.helpers import combine_metadata

# Combine all tile_metadata.csv files from PANDA slides.
combined = combine_metadata(&quot;./PANDA/tiles/&quot;, &quot;tile_metadata.csv&quot;)
print(&quot;PANDA dataset has {:.1f} million tiles.&quot;.format(len(combined) / 1e6))

# Let's also add a column which tells us where the data came from...
train_info = pandas.read_csv(&quot;./PANDA/raw/train.csv&quot;)
provider = dict(zip(train_info.image_id, train_info.data_provider))
combined[&quot;data_provider&quot;] = [provider[x] for x in combined.slide_name]

# ... and an outlier column where we mark outliers!
combined[&quot;outlier&quot;] = False
</code></pre>
<pre><code>Combining metadata: 10615it [00:55]
PANDA dataset has 2.8 million tiles.
</code></pre>
<p>Thats a lot of tiles! Luckily we don't have go through them manually... Let's start by taking a look at the UMAP representation of the preprocessing metrics.</p>
<blockquote>
<p>UMAP representation requires that install <code>umap-learn</code> with <code>pip install umap-learn</code>.</p>
</blockquote>
<pre><code class="language-python">from histoprep import OutlierDetector

# Initialize outlier detector.
detector = OutlierDetector(combined, num_clusters=10)
# Get an UMAP representation with 200k random tiles.
coords, indices = detector.umap_representation(verbose=False, max_samples=200_000)
# Plot!
detector.plot_representation(coords, indices)
</code></pre>
<p><img alt="png" src="../output_11_0.png" /></p>
<p>In the representation, each tile is annotated by it's log(distance) from the origo (the "mean" tile), and thus tiles further from the origo are more likely outliers. We can see that the tiles cluster into three bigger groups, which is due to the fact that the <code>PANDA</code> dataset originates from two different medical centres, and is scanned with three scanners.</p>
<blockquote>
<p>Histological images vary widely due to differences in scanning equipment and sample preparation. Thus, preprocessing is easier if it's done in a per-dataset basis.</p>
</blockquote>
<p>From here on we'll only process the tiles originating from Radboud University medical centre (only one scanner).</p>
<pre><code class="language-python">radboud = combined[combined.data_provider == &quot;radboud&quot;]
print(&quot;There are {:.1f} million tiles from Radboud!&quot;.format(len(radboud) / 1e6))

# Let's visualise the representation again.
detector = OutlierDetector(radboud, num_clusters=10)
coords, indices = detector.umap_representation(verbose=False, max_samples=200_000)
detector.plot_representation(coords, indices)
</code></pre>
<pre><code>There are 1.3 million tiles from Radboud!
</code></pre>
<p><img alt="png" src="../output_13_1.png" /></p>
<p>Now the representation looks a lot nicer! We can see that there are clear outlier groups, which should be easy to discard. Before we go into automatic outlier detection, we should mark some easy outliers with the <code>OutlierVisualiser</code>.</p>
<pre><code class="language-python">from histoprep import OutlierVisualizer

# We'll get a warning here as the dataset contains over a million tiles.
visualizer = OutlierVisualizer(radboud)
</code></pre>
<pre><code>/data/jopo/HistoPrep/histoprep/_outliers/_visualize.py:41: UserWarning: Plotting functions may take a while due to size of the data.
  warnings.warn(
</code></pre>
<pre><code class="language-python"># Plot the 'background' column with example tiles.
visualizer.plot_histogram_with_examples(&quot;background&quot;, log_scale=False)
</code></pre>
<p><img alt="png" src="../output_16_0.png" /></p>
<p>Here we can see that quite many of the tiles contain large amounts of background. This is not good if you're using the <a href="https://pytorch.org/vision/main/generated/torchvision.transforms.RandomResizedCrop.html"><code>RandomResizedCrop</code></a> function during neural network training to make the network scale invariant. Let's mark tiles with over 60% background as outliers and plot some other preprocessing columns.</p>
<pre><code class="language-python">radboud.loc[radboud.background &gt; 0.6, &quot;outlier&quot;] = True
</code></pre>
<pre><code class="language-python">visualizer = OutlierVisualizer(radboud[~radboud.outlier])
# Let's plot mean values from the brightness image channels.
visualizer.plot_histogram_with_examples(&quot;brightness_mean&quot;)
</code></pre>
<p><img alt="png" src="../output_19_0.png" /></p>
<p>Here we can easily detect some outliers! There are a lot of completely dark tiles, and only some tiles with weird blue shit (probably pen markings). Let's mark the completely black tiles as outliers and try to extract the blue tiles.</p>
<pre><code class="language-python">radboud.loc[radboud.brightness_mean &lt; 50, &quot;outlier&quot;] = True
</code></pre>
<pre><code class="language-python">visualizer.plot_histogram_with_examples(&quot;saturation_mean&quot;)
</code></pre>
<p><img alt="png" src="../output_22_0.png" /></p>
<p>The mean value of the saturation channel seemed to pick up the weird blue tiles. Let's mark these as outliers and move onto automatic outlier detection!</p>
<pre><code class="language-python">radboud.loc[radboud.saturation_mean &gt; 175, &quot;outlier&quot;] = True
</code></pre>
<pre><code class="language-python">from histoprep import OutlierDetector

# Include only non outliers.
detector = OutlierDetector(radboud[~radboud.outlier])
print(detector)
</code></pre>
<pre><code>OutlierDetector(num_clusters=20):
   0:  dist=36.72   images=1703
   1:  dist=25.77   images=3000
   2:  dist=16.73   images=5635
   3:  dist=16.59   images=4845
   4:  dist=10.19   images=10231
   5:  dist=8.54    images=9379
   6:  dist=7.53    images=18558
   7:  dist=6.89    images=14857
   8:  dist=6.81    images=23800
   9:  dist=6.80    images=41217
  10:  dist=4.21    images=66061
       ...
</code></pre>
<p>Here we can see that there are several clusters pretty far away from the origo. Let's visualise some random tiles from these clusters!</p>
<pre><code class="language-python">detector.plot_clusters(min_distance=8)
</code></pre>
<p><img alt="png" src="../output_27_0.png" /></p>
<p><img alt="png" src="../output_27_1.png" /></p>
<p><img alt="png" src="../output_27_2.png" /></p>
<p><img alt="png" src="../output_27_3.png" /></p>
<p><img alt="png" src="../output_27_4.png" /></p>
<p><img alt="png" src="../output_27_5.png" /></p>
<p>Here we can clearly see that <code>OutlierDetector</code> organises the clusters from most likely to least likely outlier. Clusters 0 to 3 are clear outliers, cluster 4 contains tissue regions with dark areas and cluster 5 contains (mostly) good quality tiles. Let's mark clusters 0 to 3 as outliers.</p>
<pre><code class="language-python">radboud[~radboud.outlier].loc[detector.clusters &lt;= 3, &quot;outlier&quot;] = True
</code></pre>
<p>Now we can repeat the automatic outlier detection with the remaning non-outlier tiles! Each iteration we might catch some outliers that were hiding inside larger groups.</p>
<blockquote>
<p>It's also possible to re-cluster a cluster which contains both good and bad tiles. This way you might separate the good tiles from the bad.
<code>python
cluster2 = OutlierDetector(radboud[~radboud.outlier][detector.clusters == 2], num_clusters=2)</code></p>
</blockquote>
<pre><code class="language-python">detector = OutlierDetector(radboud[~radboud.outlier])
detector
</code></pre>
<pre><code>OutlierDetector(num_clusters=20):
   0:  dist=32.15   images=3526
   1:  dist=17.48   images=5867
   2:  dist=15.59   images=6904
   3:  dist=9.39    images=4305
   4:  dist=9.24    images=16177
   5:  dist=8.65    images=8493
   6:  dist=6.67    images=18028
   7:  dist=6.67    images=25224
   8:  dist=6.58    images=19228
   9:  dist=5.92    images=67838
  10:  dist=4.34    images=40384
       ...
</code></pre>
<pre><code class="language-python">detector.plot_clusters(min_distance=10)
</code></pre>
<p><img alt="png" src="../output_32_0.png" /></p>
<p><img alt="png" src="../output_32_1.png" /></p>
<p><img alt="png" src="../output_32_2.png" /></p>
<p>Here we can see that there were still outliers not picked up during the first iteration. Let's marks clusters 0 and 1 as outliers. You could repeat this a few more times.</p>
<h1 id="what-to-do-with-the-outliers">What to do with the outliers?</h1>
<p>Now that you've uncovered outliers in the PANDA dataset, we have several options for dealing with them.</p>
<ol>
<li>Remove outliers from the dataset.</li>
<li>Label outliers as negative samples.</li>
</ol>
<p>The first option might be the easiest, but could lead to a neural network which might not do so well on a real-world dataset! As the outliers came from real-world dataset, we should include them into the training (and validation) dataset. </p>
<p>Each tile can be labeled, for example, as <code>cancer</code> vs. <code>benign</code> based on the Gleason score masks inside <code>train_label_masks</code> folder. After labeling, every tile identified as an outlier during pre-processing could be marked as <code>benign</code>. This would help the network learn to label these outliers as <code>benign</code> and not panic when encoutering these images during evaluation!</p>
              
            </div>
          </div><footer>
    <div class="rst-footer-buttons" role="navigation" aria-label="Footer Navigation">
        <a href="../../preprocess/preprocess/" class="btn btn-neutral float-left" title="Preprocessing saved tile images"><span class="icon icon-circle-arrow-left"></span> Previous</a>
        <a href="../../../api/" class="btn btn-neutral float-right" title="histoprep">Next <span class="icon icon-circle-arrow-right"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <!-- Copyright etc -->
  </div>

  Built with <a href="https://www.mkdocs.org/">MkDocs</a> using a <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
</footer>
          
        </div>
      </div>

    </section>

  </div>

  <div class="rst-versions" role="note" aria-label="Versions">
  <span class="rst-current-version" data-toggle="rst-current-version">
    
    
      <span><a href="../../preprocess/preprocess/" style="color: #fcfcfc">&laquo; Previous</a></span>
    
    
      <span><a href="../../../api/" style="color: #fcfcfc">Next &raquo;</a></span>
    
  </span>
</div>
    <script>var base_url = '../../..';</script>
    <script src="../../../js/theme_extra.js" defer></script>
    <script src="../../../js/theme.js" defer></script>
      <script src="../../../js/mkapi.js" defer></script>
      <script src="../../../search/main.js" defer></script>
    <script defer>
        window.onload = function () {
            SphinxRtdTheme.Navigation.enable(true);
        };
    </script>

</body>
</html>
